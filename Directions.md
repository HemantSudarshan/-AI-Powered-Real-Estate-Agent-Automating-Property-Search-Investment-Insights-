# ğŸ—ï¸ AI-POWERED REAL ESTATE AGENT - COMPLETE PRODUCT WORKFLOW & LLM TRANSFORMATION GUIDE

## ğŸ“‹ EXECUTIVE SUMMARY

**Current State**: Basic prototype with monolithic architecture, hardcoded credentials, no error handling
**Target State**: Production-ready AI engineering project suitable for 8-20+ LPA roles
**Timeline**: 30 days (5 hours/day)
**Skill Level Required**: Intermediate Python â†’ Advanced AI Engineering

---

## ğŸ¯ PRODUCT VISION & ARCHITECTURE

### Product Description
An intelligent real estate assistant that automates property discovery, analysis, and investment insights using multi-agent AI architecture, semantic search, and real-time market data integration.

### Core Value Propositions
1. **Time Efficiency**: Reduces property search from days to minutes
2. **Intelligence**: AI-powered insights beyond simple listings
3. **Automation**: Handles scraping, analysis, and recommendations autonomously
4. **Scalability**: Supports multiple markets and property types

### Target Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Gateway Layer                        â”‚
â”‚                    (FastAPI + Rate Limiting)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                             â”‚                           â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Search   â”‚          â”‚   Investment    â”‚      â”‚   Property        â”‚
â”‚   Agent    â”‚          â”‚   Analyzer      â”‚      â”‚   Scraper         â”‚
â”‚            â”‚          â”‚   Agent         â”‚      â”‚   Agent           â”‚
â”‚ (Semantic) â”‚          â”‚   (Predictive)  â”‚      â”‚   (Multi-source)  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                             â”‚                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Vector Database  â”‚      â”‚   Redis Cache    â”‚
            â”‚  (ChromaDB/Pinecone) â”‚   â”‚   (Hot Data)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚          PostgreSQL Database                  â”‚
            â”‚  (Properties, Users, Searches, Analytics)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”´ CRITICAL GAPS IN CURRENT IMPLEMENTATION

### 1. **Architecture Issues**
- âŒ Monolithic single-file structure
- âŒ No separation of concerns
- âŒ Hardcoded configurations
- âŒ No dependency injection
- âŒ Tight coupling between components

### 2. **Security Vulnerabilities**
- ğŸš¨ **CRITICAL**: API keys hardcoded in source code
- ğŸš¨ **CRITICAL**: No input validation or sanitization
- âŒ No authentication/authorization layer
- âŒ No rate limiting
- âŒ Exposed internal errors to users

### 3. **Data Management**
- âŒ No database persistence
- âŒ No caching layer
- âŒ No data validation schemas
- âŒ No migration management
- âŒ Inefficient in-memory storage

### 4. **AI/ML Gaps**
- âŒ No vector embeddings for semantic search
- âŒ Basic keyword matching only
- âŒ No context retention across queries
- âŒ No model versioning
- âŒ No prompt engineering framework

### 5. **Quality Assurance**
- âŒ Zero test coverage
- âŒ No CI/CD pipeline
- âŒ No logging or monitoring
- âŒ No error tracking (Sentry/Datadog)
- âŒ No performance metrics

### 6. **Documentation**
- âŒ Minimal README
- âŒ No API documentation
- âŒ No architecture diagrams
- âŒ No contribution guidelines
- âŒ No deployment guide

---

## ğŸ“ PRODUCTION-GRADE PROJECT STRUCTURE

```
real-estate-ai-agent/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                    # Automated testing
â”‚       â”œâ”€â”€ cd.yml                    # Automated deployment
â”‚       â””â”€â”€ code-quality.yml          # Linting, formatting
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                       # AI Agent implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_agent.py            # Abstract base class
â”‚   â”‚   â”œâ”€â”€ search_agent.py          # Property search logic
â”‚   â”‚   â”œâ”€â”€ investment_agent.py      # ROI analysis
â”‚   â”‚   â””â”€â”€ scraper_agent.py         # Web scraping
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                          # FastAPI application
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ properties.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py
â”‚   â”‚   â”‚   â””â”€â”€ analytics.py
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rate_limit.py
â”‚   â”‚   â”‚   â””â”€â”€ error_handler.py
â”‚   â”‚   â””â”€â”€ schemas/                 # Pydantic models
â”‚   â”‚       â”œâ”€â”€ property.py
â”‚   â”‚       â”œâ”€â”€ search.py
â”‚   â”‚       â””â”€â”€ user.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                     # Business logic layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ property_service.py
â”‚   â”‚   â”œâ”€â”€ scraping_service.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â””â”€â”€ cache_service.py
â”‚   â”‚
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py                # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ session.py               # DB connection
â”‚   â”‚   â””â”€â”€ migrations/              # Alembic migrations
â”‚   â”‚
â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chromadb_client.py
â”‚   â”‚   â””â”€â”€ embeddings.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py                # Structured logging
â”‚   â”‚   â”œâ”€â”€ config.py                # Configuration management
â”‚   â”‚   â””â”€â”€ exceptions.py            # Custom exceptions
â”‚   â”‚
â”‚   â””â”€â”€ prompts/                      # LLM prompt templates
â”‚       â”œâ”€â”€ search_prompts.py
â”‚       â”œâ”€â”€ analysis_prompts.py
â”‚       â””â”€â”€ investment_prompts.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”‚   â”œâ”€â”€ test_services.py
â”‚   â”‚   â””â”€â”€ test_utils.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_api.py
â”‚   â”‚   â””â”€â”€ test_database.py
â”‚   â””â”€â”€ e2e/
â”‚       â””â”€â”€ test_workflows.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Original scraped data
â”‚   â”œâ”€â”€ processed/                    # Cleaned data
â”‚   â””â”€â”€ embeddings/                   # Cached embeddings
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md                        # API documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md               # System design
â”‚   â”œâ”€â”€ DEPLOYMENT.md                 # Deployment guide
â”‚   â””â”€â”€ CONTRIBUTING.md               # Contribution guide
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_db.py
â”‚   â”œâ”€â”€ seed_data.py
â”‚   â””â”€â”€ migrate.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ Dockerfile.dev
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ .env.example                      # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml           # Git hooks
â”œâ”€â”€ pyproject.toml                    # Poetry config
â”œâ”€â”€ requirements.txt                  # Pip dependencies
â”œâ”€â”€ README.md                         # Comprehensive docs
â””â”€â”€ LICENSE
```

---

## ğŸ¤– LLM INSTRUCTIONS: STEP-BY-STEP TRANSFORMATION

### PHASE 1: FOUNDATION (Days 1-7)

#### Step 1.1: Project Restructuring
**LLM Task**: "Reorganize the current monolithic code into the production structure above"

**Actions**:
1. Create directory structure exactly as specified
2. Move existing code into appropriate modules
3. Separate concerns: agents, services, API, database
4. Create `__init__.py` files for all packages
5. Update all import statements

**Validation**:
```bash
# Directory structure should match exactly
tree src/ -L 2
# All imports should resolve without errors
python -c "from src.agents import search_agent"
```

#### Step 1.2: Environment Configuration
**LLM Task**: "Extract all hardcoded values into environment configuration"

**Create `.env.example`**:
```bash
# API Keys
OPENAI_API_KEY=your_openai_key_here
GEMINI_API_KEY=your_gemini_key_here
FIRECRAWL_API_KEY=your_firecrawl_key_here

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/real_estate
REDIS_URL=redis://localhost:6379/0

# Vector Store
CHROMADB_PATH=./data/chromadb
EMBEDDING_MODEL=text-embedding-3-small

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
RATE_LIMIT=100/minute

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/app.log

# Feature Flags
ENABLE_WEB_SCRAPING=true
ENABLE_CACHING=true
ENABLE_VECTOR_SEARCH=true
```

**Create `src/utils/config.py`**:
```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # API Keys
    openai_api_key: str
    gemini_api_key: str
    firecrawl_api_key: str
    
    # Database
    database_url: str
    redis_url: str
    
    # Vector Store
    chromadb_path: str = "./data/chromadb"
    embedding_model: str = "text-embedding-3-small"
    
    # API Config
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    api_workers: int = 4
    rate_limit: str = "100/minute"
    
    # Logging
    log_level: str = "INFO"
    log_file: str = "./logs/app.log"
    
    class Config:
        env_file = ".env"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    return Settings()
```

#### Step 1.3: Type Annotations & Schemas
**LLM Task**: "Add comprehensive type hints and Pydantic schemas"

**Example Schema (`src/api/schemas/property.py`)**:
```python
from pydantic import BaseModel, Field, validator
from typing import Optional, List
from datetime import datetime
from enum import Enum

class PropertyType(str, Enum):
    APARTMENT = "apartment"
    HOUSE = "house"
    VILLA = "villa"
    PLOT = "plot"
    COMMERCIAL = "commercial"

class PropertyStatus(str, Enum):
    AVAILABLE = "available"
    SOLD = "sold"
    RENTED = "rented"
    PENDING = "pending"

class PropertyBase(BaseModel):
    title: str = Field(..., min_length=10, max_length=200)
    description: str = Field(..., min_length=50)
    property_type: PropertyType
    price: float = Field(..., gt=0)
    location: str
    city: str
    state: str
    pincode: str = Field(..., regex=r'^\d{6}$')
    bedrooms: Optional[int] = Field(None, ge=0, le=20)
    bathrooms: Optional[int] = Field(None, ge=0, le=20)
    area_sqft: float = Field(..., gt=0)
    year_built: Optional[int] = Field(None, ge=1800, le=2025)

class PropertyCreate(PropertyBase):
    source_url: str
    listing_date: datetime = Field(default_factory=datetime.now)

class PropertyResponse(PropertyBase):
    id: int
    status: PropertyStatus
    created_at: datetime
    updated_at: datetime
    embedding_id: Optional[str] = None
    investment_score: Optional[float] = Field(None, ge=0, le=100)
    
    class Config:
        orm_mode = True

class PropertySearchRequest(BaseModel):
    query: str = Field(..., min_length=3, max_length=500)
    property_type: Optional[List[PropertyType]] = None
    min_price: Optional[float] = Field(None, ge=0)
    max_price: Optional[float] = Field(None, ge=0)
    city: Optional[str] = None
    min_bedrooms: Optional[int] = Field(None, ge=0)
    max_bedrooms: Optional[int] = Field(None, le=20)
    min_area: Optional[float] = Field(None, ge=0)
    max_area: Optional[float] = None
    limit: int = Field(10, ge=1, le=100)
    
    @validator('max_price')
    def validate_price_range(cls, v, values):
        if 'min_price' in values and values['min_price'] and v:
            if v < values['min_price']:
                raise ValueError('max_price must be >= min_price')
        return v
```

---

### PHASE 2: CORE FUNCTIONALITY (Days 8-14)

#### Step 2.1: Database Layer
**LLM Task**: "Implement SQLAlchemy models and database management"

**Database Models (`src/database/models.py`)**:
```python
from sqlalchemy import Column, Integer, String, Float, DateTime, Enum, Text, Index
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
from datetime import datetime
import enum

Base = declarative_base()

class PropertyTypeEnum(enum.Enum):
    APARTMENT = "apartment"
    HOUSE = "house"
    VILLA = "villa"
    PLOT = "plot"
    COMMERCIAL = "commercial"

class Property(Base):
    __tablename__ = "properties"
    
    id = Column(Integer, primary_key=True, index=True)
    title = Column(String(200), nullable=False)
    description = Column(Text, nullable=False)
    property_type = Column(Enum(PropertyTypeEnum), nullable=False)
    price = Column(Float, nullable=False)
    
    # Location
    location = Column(String(200), nullable=False)
    city = Column(String(100), nullable=False, index=True)
    state = Column(String(100), nullable=False)
    pincode = Column(String(6), nullable=False)
    latitude = Column(Float, nullable=True)
    longitude = Column(Float, nullable=True)
    
    # Property details
    bedrooms = Column(Integer, nullable=True)
    bathrooms = Column(Integer, nullable=True)
    area_sqft = Column(Float, nullable=False)
    year_built = Column(Integer, nullable=True)
    
    # Source tracking
    source = Column(String(100), nullable=False)  # 99acres, housing.com, etc.
    source_url = Column(Text, nullable=False)
    listing_date = Column(DateTime, nullable=False)
    
    # AI/ML
    embedding_id = Column(String(100), nullable=True, index=True)
    investment_score = Column(Float, nullable=True)
    
    # Metadata
    status = Column(String(50), default="available")
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
    
    # Indexes for performance
    __table_args__ = (
        Index('idx_city_price', 'city', 'price'),
        Index('idx_type_city', 'property_type', 'city'),
        Index('idx_created_at', 'created_at'),
    )

class SearchHistory(Base):
    __tablename__ = "search_history"
    
    id = Column(Integer, primary_key=True)
    user_id = Column(String(100), nullable=True)
    query = Column(Text, nullable=False)
    filters = Column(Text, nullable=True)  # JSON
    results_count = Column(Integer, nullable=False)
    search_time_ms = Column(Integer, nullable=True)
    created_at = Column(DateTime, server_default=func.now())

class InvestmentAnalysis(Base):
    __tablename__ = "investment_analysis"
    
    id = Column(Integer, primary_key=True)
    property_id = Column(Integer, index=True)
    roi_5year = Column(Float, nullable=True)
    roi_10year = Column(Float, nullable=True)
    appreciation_rate = Column(Float, nullable=True)
    rental_yield = Column(Float, nullable=True)
    risk_score = Column(Float, nullable=True)
    analysis_json = Column(Text, nullable=True)  # Full analysis
    created_at = Column(DateTime, server_default=func.now())
```

**Database Session (`src/database/session.py`)**:
```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from contextlib import contextmanager
from src.utils.config import get_settings

settings = get_settings()

engine = create_engine(
    settings.database_url,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db() -> Session:
    """Dependency for FastAPI"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@contextmanager
def get_db_context():
    """Context manager for scripts"""
    db = SessionLocal()
    try:
        yield db
        db.commit()
    except Exception:
        db.rollback()
        raise
    finally:
        db.close()
```

#### Step 2.2: Vector Database Integration
**LLM Task**: "Implement semantic search using ChromaDB/Pinecone"

**Vector Store (`src/vectorstore/chromadb_client.py`)**:
```python
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Optional
from src.utils.config import get_settings
from src.utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()

class VectorStore:
    def __init__(self):
        self.client = chromadb.PersistentClient(
            path=settings.chromadb_path,
            settings=Settings(anonymized_telemetry=False)
        )
        self.collection = self.client.get_or_create_collection(
            name="properties",
            metadata={"hnsw:space": "cosine"}
        )
    
    def add_property(
        self,
        property_id: int,
        embedding: List[float],
        metadata: Dict
    ) -> str:
        """Add property embedding to vector store"""
        try:
            embedding_id = f"prop_{property_id}"
            self.collection.add(
                ids=[embedding_id],
                embeddings=[embedding],
                metadatas=[metadata]
            )
            logger.info(f"Added property {property_id} to vector store")
            return embedding_id
        except Exception as e:
            logger.error(f"Failed to add property to vector store: {e}")
            raise
    
    def search(
        self,
        query_embedding: List[float],
        n_results: int = 10,
        filter_dict: Optional[Dict] = None
    ) -> List[Dict]:
        """Semantic search for properties"""
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=filter_dict
            )
            
            return [{
                'id': id_,
                'metadata': meta,
                'distance': dist
            } for id_, meta, dist in zip(
                results['ids'][0],
                results['metadatas'][0],
                results['distances'][0]
            )]
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []
    
    def delete_property(self, property_id: int):
        """Remove property from vector store"""
        embedding_id = f"prop_{property_id}"
        self.collection.delete(ids=[embedding_id])
```

**Embedding Service (`src/services/embedding_service.py`)**:
```python
from openai import OpenAI
from typing import List
from src.utils.config import get_settings
from src.utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()

class EmbeddingService:
    def __init__(self):
        self.client = OpenAI(api_key=settings.openai_api_key)
        self.model = settings.embedding_model
    
    def create_embedding(self, text: str) -> List[float]:
        """Generate embedding for text"""
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise
    
    def create_property_text(self, property_data: Dict) -> str:
        """Convert property data to searchable text"""
        parts = [
            property_data.get('title', ''),
            property_data.get('description', ''),
            f"{property_data.get('bedrooms', 0)} bedroom",
            f"{property_data.get('property_type', '')}",
            f"in {property_data.get('city', '')}",
            f"{property_data.get('area_sqft', 0)} sqft"
        ]
        return " ".join(filter(None, parts))
```

#### Step 2.3: Error Handling & Logging
**LLM Task**: "Implement comprehensive error handling and structured logging"

**Logger (`src/utils/logger.py`)**:
```python
import logging
import sys
from pathlib import Path
from src.utils.config import get_settings

settings = get_settings()

def setup_logger():
    """Configure structured logging"""
    log_file = Path(settings.log_file)
    log_file.parent.mkdir(parents=True, exist_ok=True)
    
    # Format
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(formatter)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    
    # Root logger
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, settings.log_level))
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def get_logger(name: str) -> logging.Logger:
    """Get logger for module"""
    return logging.getLogger(name)
```

**Custom Exceptions (`src/utils/exceptions.py`)**:
```python
class RealEstateAgentException(Exception):
    """Base exception"""
    def __init__(self, message: str, error_code: str = None):
        self.message = message
        self.error_code = error_code
        super().__init__(self.message)

class PropertyNotFoundError(RealEstateAgentException):
    """Property doesn't exist"""
    def __init__(self, property_id: int):
        super().__init__(
            f"Property {property_id} not found",
            error_code="PROPERTY_NOT_FOUND"
        )

class ScrapingError(RealEstateAgentException):
    """Web scraping failed"""
    def __init__(self, source: str, reason: str):
        super().__init__(
            f"Failed to scrape {source}: {reason}",
            error_code="SCRAPING_FAILED"
        )

class EmbeddingError(RealEstateAgentException):
    """Embedding generation failed"""
    def __init__(self, reason: str):
        super().__init__(
            f"Embedding generation failed: {reason}",
            error_code="EMBEDDING_FAILED"
        )

class DatabaseError(RealEstateAgentException):
    """Database operation failed"""
    pass
```

---

### PHASE 3: API & AGENTS (Days 15-21)

#### Step 3.1: FastAPI Application
**LLM Task**: "Build production-ready REST API with FastAPI"

**Main App (`src/api/main.py`)**:
```python
from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import time

from src.api.routes import properties, search, analytics
from src.api.middleware.rate_limit import RateLimitMiddleware
from src.utils.logger import setup_logger, get_logger
from src.utils.exceptions import RealEstateAgentException
from src.database.session import engine
from src.database.models import Base

logger = get_logger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Startup and shutdown events"""
    # Startup
    setup_logger()
    logger.info("Starting Real Estate AI Agent API")
    Base.metadata.create_all(bind=engine)
    yield
    # Shutdown
    logger.info("Shutting down Real Estate AI Agent API")

app = FastAPI(
    title="Real Estate AI Agent",
    description="Intelligent property search and investment analysis",
    version="1.0.0",
    lifespan=lifespan
)

# Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(RateLimitMiddleware)

# Exception handlers
@app.exception_handler(RealEstateAgentException)
async def custom_exception_handler(request: Request, exc: RealEstateAgentException):
    return JSONResponse(
        status_code=status.HTTP_400_BAD_REQUEST,
        content={
            "error": exc.error_code,
            "message": exc.message,
            "timestamp": time.time()
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": "INTERNAL_ERROR",
            "message": "An internal error occurred",
            "timestamp": time.time()
        }
    )

# Routes
app.include_router(properties.router, prefix="/api/v1/properties", tags=["Properties"])
app.include_router(search.router, prefix="/api/v1/search", tags=["Search"])
app.include_router(analytics.router, prefix="/api/v1/analytics", tags=["Analytics"])

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": "1.0.0",
        "timestamp": time.time()
    }
```

**Search Endpoint (`src/api/routes/search.py`)**:
```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List

from src.api.schemas.property import PropertySearchRequest, PropertyResponse
from src.database.session import get_db
from src.services.search_service import SearchService
from src.utils.logger import get_logger

router = APIRouter()
logger = get_logger(__name__)

@router.post("/semantic", response_model=List[PropertyResponse])
async def semantic_search(
    request: PropertySearchRequest,
    db: Session = Depends(get_db)
):
    """
    Semantic property search using AI embeddings
    
    - **query**: Natural language search query
    - **property_type**: Filter by property types
    - **min_price/max_price**: Price range filter
    - **city**: City filter
    - **limit**: Number of results (max 100)
    """
    try:
        service = SearchService(db)
        results = await service.semantic_search(
            query=request.query,
            filters={
                'property_type': request.property_type,
                'min_price': request.min_price,
                'max_price': request.max_price,
                'city': request.city,
                'min_bedrooms': request.min_bedrooms,
                'max_bedrooms': request.max_bedrooms,
            },
            limit=request.limit
        )
        
        logger.info(f"Semantic search returned {len(results)} results")
        return results
        
    except Exception as e:
        logger.error(f"Search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

#### Step 3.2: AI Agents Implementation
**LLM Task**: "Implement modular AI agents with LangGraph"

**Base Agent (`src/agents/base_agent.py`)**:
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from src.utils.logger import get_logger

class BaseAgent(ABC):
    """Abstract base class for all AI agents"""
    
    def __init__(self, name: str, model: str = "gpt-4"):
        self.name = name
        self.model = model
        self.logger = get_logger(f"agents.{name}")
    
    @abstractmethod
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute agent logic"""
        pass
    
    def log_execution(self, input_data: Dict, output_data: Dict):
        """Log agent execution"""
        self.logger.info(
            f"Agent {self.name} executed",
            extra={
                'input': str(input_data)[:200],
                'output': str(output_data)[:200]
            }
        )

**Investment Analyzer Agent (`src/agents/investment_agent.py`)**:
```python
from typing import Dict, Any
import openai
from src.agents.base_agent import BaseAgent
from src.utils.config import get_settings

settings = get_settings()

class InvestmentAnalyzerAgent(BaseAgent):
    """Analyzes investment potential of properties"""
    
    def __init__(self):
        super().__init__(name="investment_analyzer", model="gpt-4")
        self.client = openai.OpenAI(api_key=settings.openai_api_key)
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze investment potential
        
        Input: {
            'property': PropertyResponse,
            'market_data': Dict (optional)
        }
        
        Output: {
            'roi_5year': float,
            'roi_10year': float,
            'appreciation_rate': float,
            'rental_yield': float,
            'risk_score': float,
            'recommendation': str,
            'analysis': str
        }
        """
        property_data = input_data['property']
        
        prompt = f"""
        Analyze the investment potential of this property:
        
        Property Details:
        - Type: {property_data.property_type}
        - Location: {property_data.city}, {property_data.state}
        - Price: â‚¹{property_data.price:,.2f}
        - Area: {property_data.area_sqft} sq ft
        - Bedrooms: {property_data.bedrooms}
        - Year Built: {property_data.year_built}
        
        Provide detailed analysis including:
        1. 5-year and 10-year ROI projections
        2. Expected appreciation rate
        3. Rental yield potential
        4. Risk assessment (0-100 scale)
        5. Investment recommendation
        
        Return analysis as structured JSON.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a real estate investment analyst with 20 years of experience in the Indian market."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            analysis = eval(response.choices[0].message.content)
            self.log_execution(input_data, analysis)
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Investment analysis failed: {e}")
            raise

**Search Agent (`src/agents/search_agent.py`)**:
```python
from typing import Dict, Any, List
import openai
from src.agents.base_agent import BaseAgent
from src.utils.config import get_settings
from src.services.embedding_service import EmbeddingService
from src.vectorstore.chromadb_client import VectorStore

settings = get_settings()

class SearchAgent(BaseAgent):
    """Handles intelligent property search"""
    
    def __init__(self):
        super().__init__(name="search_agent", model="gpt-4")
        self.embedding_service = EmbeddingService()
        self.vector_store = VectorStore()
        self.client = openai.OpenAI(api_key=settings.openai_api_key)
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute semantic search
        
        Input: {
            'query': str,
            'filters': Dict,
            'limit': int
        }
        
        Output: {
            'properties': List[Dict],
            'search_summary': str,
            'recommendations': List[str]
        }
        """
        query = input_data['query']
        filters = input_data.get('filters', {})
        limit = input_data.get('limit', 10)
        
        # Generate embedding
        query_embedding = self.embedding_service.create_embedding(query)
        
        # Search vector store
        results = self.vector_store.search(
            query_embedding=query_embedding,
            n_results=limit,
            filter_dict=self._build_filter(filters)
        )
        
        # Enhance with AI summary
        summary = await self._generate_summary(query, results)
        
        return {
            'properties': results,
            'search_summary': summary,
            'recommendations': await self._generate_recommendations(results)
        }
    
    def _build_filter(self, filters: Dict) -> Dict:
        """Convert API filters to ChromaDB format"""
        chroma_filter = {}
        
        if 'city' in filters and filters['city']:
            chroma_filter['city'] = filters['city']
        
        if 'property_type' in filters and filters['property_type']:
            chroma_filter['property_type'] = {'$in': filters['property_type']}
        
        return chroma_filter
    
    async def _generate_summary(self, query: str, results: List[Dict]) -> str:
        """Generate AI summary of search results"""
        prompt = f"""
        User searched for: "{query}"
        
        Found {len(results)} matching properties.
        
        Provide a brief 2-3 sentence summary of the search results,
        highlighting key patterns and notable properties.
        """
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.5
        )
        
        return response.choices[0].message.content
    
    async def _generate_recommendations(self, results: List[Dict]) -> List[str]:
        """Generate personalized recommendations"""
        # Implement recommendation logic
        return [
            "Consider properties in emerging neighborhoods",
            "Properties with 3+ bedrooms show better appreciation",
            "Look for properties near metro stations"
        ]
```

---

### PHASE 4: TESTING & QUALITY (Days 22-25)

#### Step 4.1: Unit Tests
**LLM Task**: "Write comprehensive unit tests with pytest"

**Test Configuration (`tests/conftest.py`)**:
```python
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from src.database.models import Base
from src.api.main import app
from fastapi.testclient import TestClient

# Test database
TEST_DATABASE_URL = "sqlite:///./test.db"

@pytest.fixture(scope="session")
def engine():
    engine = create_engine(TEST_DATABASE_URL)
    Base.metadata.create_all(bind=engine)
    yield engine
    Base.metadata.drop_all(bind=engine)

@pytest.fixture(scope="function")
def db_session(engine):
    TestingSessionLocal = sessionmaker(bind=engine)
    session = TestingSessionLocal()
    yield session
    session.rollback()
    session.close()

@pytest.fixture
def client():
    return TestClient(app)

@pytest.fixture
def sample_property():
    return {
        "title": "Spacious 3BHK Apartment",
        "description": "Beautiful apartment with modern amenities in prime location",
        "property_type": "apartment",
        "price": 5000000,
        "location": "Indiranagar",
        "city": "Bangalore",
        "state": "Karnataka",
        "pincode": "560038",
        "bedrooms": 3,
        "bathrooms": 2,
        "area_sqft": 1500,
        "year_built": 2020,
        "source": "99acres",
        "source_url": "https://example.com/property/123"
    }
```

**Agent Tests (`tests/unit/test_agents.py`)**:
```python
import pytest
from src.agents.investment_agent import InvestmentAnalyzerAgent
from src.agents.search_agent import SearchAgent

@pytest.mark.asyncio
async def test_investment_analyzer_basic():
    agent = InvestmentAnalyzerAgent()
    
    input_data = {
        'property': {
            'property_type': 'apartment',
            'city': 'Mumbai',
            'state': 'Maharashtra',
            'price': 10000000,
            'area_sqft': 1200,
            'bedrooms': 2,
            'year_built': 2022
        }
    }
    
    result = await agent.execute(input_data)
    
    assert 'roi_5year' in result
    assert 'roi_10year' in result
    assert 'risk_score' in result
    assert 0 <= result['risk_score'] <= 100

@pytest.mark.asyncio
async def test_search_agent_semantic():
    agent = SearchAgent()
    
    input_data = {
        'query': '3 bedroom apartment near IT park',
        'limit': 5
    }
    
    result = await agent.execute(input_data)
    
    assert 'properties' in result
    assert 'search_summary' in result
    assert len(result['properties']) <= 5

def test_search_agent_filter_building():
    agent = SearchAgent()
    
    filters = {
        'city': 'Bangalore',
        'property_type': ['apartment', 'villa'],
        'min_price': 5000000,
        'max_price': 10000000
    }
    
    chroma_filter = agent._build_filter(filters)
    
    assert chroma_filter['city'] == 'Bangalore'
    assert '$in' in chroma_filter['property_type']
```

**API Tests (`tests/integration/test_api.py`)**:
```python
from fastapi.testclient import TestClient

def test_health_check(client):
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()['status'] == 'healthy'

def test_create_property(client, sample_property):
    response = client.post("/api/v1/properties/", json=sample_property)
    assert response.status_code == 201
    data = response.json()
    assert data['title'] == sample_property['title']
    assert 'id' in data

def test_search_properties(client):
    search_request = {
        "query": "apartment in Bangalore",
        "limit": 10
    }
    
    response = client.post("/api/v1/search/semantic", json=search_request)
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)

def test_invalid_search_request(client):
    search_request = {
        "query": "ab",  # Too short
        "limit": 10
    }
    
    response = client.post("/api/v1/search/semantic", json=search_request)
    assert response.status_code == 422  # Validation error

def test_property_not_found(client):
    response = client.get("/api/v1/properties/99999")
    assert response.status_code == 404
```

#### Step 4.2: Integration Tests
**Database Integration (`tests/integration/test_database.py`)**:
```python
from src.database.models import Property, PropertyTypeEnum
from datetime import datetime

def test_create_property(db_session, sample_property):
    property_obj = Property(**sample_property, listing_date=datetime.now())
    db_session.add(property_obj)
    db_session.commit()
    
    assert property_obj.id is not None
    assert property_obj.city == sample_property['city']

def test_query_by_city(db_session):
    # Create test properties
    for city in ['Mumbai', 'Bangalore', 'Delhi']:
        prop = Property(
            title=f"Test Property in {city}",
            description="Test",
            property_type=PropertyTypeEnum.APARTMENT,
            price=5000000,
            location="Test Location",
            city=city,
            state="Test State",
            pincode="560001",
            area_sqft=1000,
            source="test",
            source_url="https://test.com",
            listing_date=datetime.now()
        )
        db_session.add(prop)
    db_session.commit()
    
    bangalore_props = db_session.query(Property).filter(
        Property.city == 'Bangalore'
    ).all()
    
    assert len(bangalore_props) == 1
    assert bangalore_props[0].city == 'Bangalore'

def test_price_range_query(db_session):
    # Test price filtering
    properties = db_session.query(Property).filter(
        Property.price.between(4000000, 6000000)
    ).all()
    
    for prop in properties:
        assert 4000000 <= prop.price <= 6000000
```

---

### PHASE 5: DEPLOYMENT & DOCUMENTATION (Days 26-30)

#### Step 5.1: Docker Configuration
**LLM Task**: "Create production-ready Docker setup"

**Dockerfile**:
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY src/ ./src/
COPY alembic.ini .

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s \
    CMD curl -f http://localhost:8000/health || exit 1

# Start application
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: real_estate
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chromadb_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE

volumes:
  postgres_data:
  redis_data:
  chromadb_data:
```

#### Step 5.2: CI/CD Pipeline
**GitHub Actions (`.github/workflows/ci.yml`)**:
```yaml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 3s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio
    
    - name: Lint with flake8
      run: |
        pip install flake8
        flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ --count --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type check with mypy
      run: |
        pip install mypy
        mypy src/
    
    - name: Run tests with coverage
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379/0
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        pytest tests/ --cov=src --cov-report=xml --cov-report=html
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
    
    - name: Security audit
      run: |
        pip install safety bandit
        safety check
        bandit -r src/
  
  build:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Build Docker image
      run: docker build -t real-estate-ai-agent:${{ github.sha }} .
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'real-estate-ai-agent:${{ github.sha }}'
        format: 'table'
        exit-code: '1'
        severity: 'CRITICAL,HIGH'
```

#### Step 5.3: Comprehensive Documentation

**README.md**:
```markdown
# ğŸ¡ AI-Powered Real Estate Agent

![CI Status](https://github.com/HemantSudarshan/real-estate-ai/workflows/CI%20Pipeline/badge.svg)
![Test Coverage](https://codecov.io/gh/HemantSudarshan/real-estate-ai/branch/main/graph/badge.svg)
![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

An intelligent real estate platform that automates property search, analysis, and investment insights using multi-agent AI architecture.

## âœ¨ Features

- ğŸ” **Semantic Search**: Natural language property search using embeddings
- ğŸ“Š **Investment Analysis**: AI-powered ROI and appreciation predictions
- ğŸ¤– **Multi-Agent System**: Specialized agents for different tasks
- âš¡ **Real-time Scraping**: Live data from 99acres, Housing.com, MagicBricks
- ğŸ’¾ **Vector Database**: Fast similarity search with ChromaDB
- ğŸ” **Production-Ready**: Comprehensive error handling, logging, testing

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- PostgreSQL 15+
- Redis 7+

### Installation

1. Clone repository:
```bash
git clone https://github.com/HemantSudarshan/real-estate-ai.git
cd real-estate-ai
```

2. Set up environment:
```bash
cp .env.example .env
# Edit .env with your API keys
```

3. Start with Docker:
```bash
docker-compose up -d
```

4. Run migrations:
```bash
docker-compose exec api alembic upgrade head
```

5. Access API:
```
http://localhost:8000/docs
```

### Development Setup

1. Create virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

3. Run locally:
```bash
uvicorn src.api.main:app --reload
```

## ğŸ“š API Documentation

### Semantic Search
```bash
POST /api/v1/search/semantic
Content-Type: application/json

{
  "query": "3 bedroom apartment near IT park under 1 crore",
  "city": "Bangalore",
  "limit": 10
}
```

### Investment Analysis
```bash
GET /api/v1/analytics/investment/{property_id}
```

Full API documentation: http://localhost:8000/docs

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FastAPI Gateway                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚                â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Search â”‚  â”‚Investmentâ”‚   â”‚ Scraper  â”‚
â”‚ Agent  â”‚  â”‚  Agent   â”‚   â”‚  Agent   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚            â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Vector DB +    â”‚
        â”‚  PostgreSQL +   â”‚
        â”‚  Redis Cache    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing

Run tests:
```bash
pytest tests/ -v --cov=src
```

Run specific test:
```bash
pytest tests/unit/test_agents.py::test_investment_analyzer
```

Coverage report:
```bash
pytest --cov=src --cov-report=html
```

## ğŸ“¦ Project Structure

```
src/
â”œâ”€â”€ agents/          # AI agent implementations
â”œâ”€â”€ api/             # FastAPI routes & schemas
â”œâ”€â”€ database/        # SQLAlchemy models & migrations
â”œâ”€â”€ services/        # Business logic layer
â”œâ”€â”€ vectorstore/     # Vector database clients
â””â”€â”€ utils/           # Utilities & config

tests/
â”œâ”€â”€ unit/            # Unit tests
â”œâ”€â”€ integration/     # Integration tests
â””â”€â”€ e2e/             # End-to-end tests
```

## ğŸ”§ Configuration

Key environment variables:

```bash
# API Keys
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=...
FIRECRAWL_API_KEY=...

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/real_estate
REDIS_URL=redis://localhost:6379/0

# Features
ENABLE_WEB_SCRAPING=true
ENABLE_CACHING=true
ENABLE_VECTOR_SEARCH=true
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

## ğŸ™ Acknowledgments

- OpenAI for GPT models
- Google for Gemini AI
- FireCrawl for web scraping
- ChromaDB for vector search

## ğŸ“§ Contact

Hemant Sudarshan - [@YourTwitter](https://twitter.com/yourhandle)

Project Link: https://github.com/HemantSudarshan/real-estate-ai
```

---

## ğŸ“ˆ SUCCESS METRICS

After completing this transformation, your project will have:

### Quantitative Improvements
- âœ… **60%+ test coverage** (from 0%)
- âœ… **500+ lines** of production code
- âœ… **20+ API endpoints** (documented)
- âœ… **<200ms** average response time
- âœ… **Zero** hardcoded secrets
- âœ… **100%** type-annotated code

### Qualitative Improvements
- âœ… Production-ready architecture
- âœ… Industry-standard error handling
- âœ… Comprehensive documentation
- âœ… CI/CD pipeline
- âœ… Docker deployment
- âœ… Professional code organization

### Career Impact
This project will demonstrate:
1. **System Design**: Multi-agent architecture
2. **Best Practices**: Testing, logging, error handling
3. **DevOps**: Docker, CI/CD, monitoring
4. **AI/ML**: Vector search, embeddings, LLM integration
5. **API Design**: RESTful, documented, production-ready

**Target Salary Range**: 8-20+ LPA roles will recognize this as production-grade work.

---

## ğŸ¯ FINAL CHECKLIST

### Before Applying to Jobs
- [ ] All 60+ tests passing
- [ ] CI/CD pipeline green
- [ ] Docker compose works end-to-end
- [ ] README has screenshots/demos
- [ ] API documentation complete
- [ ] Code coverage >60%
- [ ] No hardcoded secrets
- [ ] Logging configured
- [ ] Error handling comprehensive
- [ ] Type hints throughout

### Portfolio Presentation
- [ ] Live demo deployed (Railway/Render/AWS)
- [ ] GitHub repository clean
- [ ] Architecture diagram in README
- [ ] Performance metrics documented
- [ ] Security audit passed
- [ ] Contribution guidelines
- [ ] License file present

---

## ğŸ’¡ NEXT STEPS FOR LLM

When implementing this transformation, the LLM should:

1. **Start with Phase 1** (Days 1-7): Foundation is critical
2. **Don't skip testing**: Tests prove production-readiness
3. **Follow structure exactly**: This is industry-standard
4. **Ask clarifying questions**: If user requirements unclear
5. **Provide code samples**: Show, don't just tell
6. **Explain trade-offs**: Why certain decisions matter
7. **Reference best practices**: Link to docs when relevant
8. **Validate each phase**: Ensure working before moving forward

This is not just refactoringâ€”it's transforming a prototype into a portfolio piece that demonstrates senior-level engineering skills.